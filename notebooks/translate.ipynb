{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re,string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A: ムーリエルは２０歳になりました。\\tMuiriel is 20 now.#ID=1282_4707\\n',\n",
       " 'B: は 二十歳(はたち){２０歳} になる[01]{になりました}\\n',\n",
       " 'A: すぐに戻ります。\\tI will be back soon.#ID=1284_4709\\n',\n",
       " 'B: 直ぐに{すぐに} 戻る{戻ります}\\n',\n",
       " 'A: すぐに諦めて昼寝をするかも知れない。\\tI may give up soon and just nap instead.#ID=1300_4727\\n']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = (path/'examples.utf').open().readlines()\n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_corpus(corpus_path):\n",
    "    corpus = corpus_path.open().readlines()\n",
    "    en,ja = [],[]\n",
    "    pat = r'#ID.+\\n'\n",
    "    for c in corpus:\n",
    "        if 'A: ' in c:\n",
    "            clean_c = c.replace('A: ','')\n",
    "            res = re.search(pat,clean_c)\n",
    "            clean_c = clean_c.replace(res.group(0),'').split('\\t')\n",
    "            ja.append(clean_c[0])\n",
    "            en.append(clean_c[1])\n",
    "    return en,ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Muiriel is 20 now.', 'I will be back soon.'],\n",
       " ['ムーリエルは２０歳になりました。', 'すぐに戻ります。'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en, ja = make_corpus(path/'examples.utf')\n",
    "en[:2],ja[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger('-Owakati')\n",
    "\n",
    "def ja_tokenizer(text):\n",
    "    result = tagger.parse(text)\n",
    "    words = result.split()\n",
    "    if len(words) ==0: return []\n",
    "    if words[-1] == '\\n':return words[:-1]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ムーリエル', 'は', '２', '０', '歳', 'に', 'なり', 'まし', 'た', '。']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ja_tokenizer(ja[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.symbols import ORTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tok = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    return [t.text for t in en_tok.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tokenizer(en[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['muiriel', 'is', '20', 'now', '.'],\n",
       "  ['i', 'will', 'be', 'back', 'soon', '.']],\n",
       " [['ムーリエル', 'は', '２', '０', '歳', 'に', 'なり', 'まし', 'た', '。'],\n",
       "  ['すぐ', 'に', '戻り', 'ます', '。']])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_toks = [en_tokenizer(text) for text in en]\n",
    "ja_toks = [ja_tokenizer(text) for text in ja]\n",
    "en_toks[:2], ja_toks[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149786, 149786)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_toks), len(ja_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter,defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_tok(tokens, max_vocab=50000, min_freq=0, unk_tok=\"_unk_\", pad_tok=\"_pad_\", bos_tok=\"_bos_\", eos_tok=\"_eos_\"):\n",
    "    if isinstance(tokens, str):\n",
    "        raise ValueError(\"Expected to receive a list of tokens. Received a string instead\")\n",
    "    if isinstance(tokens[0], list):\n",
    "        tokens = [p for o in tokens for p in o]\n",
    "    freq = Counter(tokens)\n",
    "    int2tok = [o for o,c in freq.most_common(max_vocab) if c>min_freq]\n",
    "    unk_id = 3\n",
    "    int2tok.insert(0, bos_tok)\n",
    "    int2tok.insert(1, pad_tok)\n",
    "    int2tok.insert(2, eos_tok)\n",
    "    int2tok.insert(unk_id, unk_tok)\n",
    "    tok2int = defaultdict(lambda:unk_id, {v:k for k,v in enumerate(int2tok)})\n",
    "    return int2tok, tok2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2j,j2int = numericalize_tok(ja_toks)\n",
    "int2en,en2int = numericalize_tok(en_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31813, 21393)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int2j), len(int2en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(int2j,(path/'int2j.pkl').open('wb'))\n",
    "pickle.dump(int2en,(path/'int2en.pkl').open('wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2j = pickle.load((path/'int2j.pkl').open('rb'))\n",
    "int2en = pickle.load((path/'int2en.pkl').open('rb'))\n",
    "j2int = defaultdict(lambda:3, {v:k for k,v in enumerate(int2j)})\n",
    "en2int = defaultdict(lambda:3, {v:k for k,v in enumerate(int2en)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31813, 21393)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int2j), len(int2en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149786,\n",
       " 149786,\n",
       " [0,\n",
       "  48,\n",
       "  6,\n",
       "  4891,\n",
       "  5,\n",
       "  109,\n",
       "  11,\n",
       "  143,\n",
       "  10,\n",
       "  83,\n",
       "  8,\n",
       "  57,\n",
       "  86,\n",
       "  1798,\n",
       "  7,\n",
       "  2146,\n",
       "  232,\n",
       "  255,\n",
       "  47,\n",
       "  36,\n",
       "  4,\n",
       "  2],\n",
       " [0, 114, 2251, 107, 38, 97, 85, 2649, 77, 28, 356, 4, 2])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j_ids = np.array([[0]+[j2int[o] for o in sent]+[2] for sent in ja_toks])\n",
    "en_ids = np.array([[0]+[en2int[o] for o in sent]+[2] for sent in en_toks])\n",
    "len(j_ids),len(en_ids), j_ids[10],en_ids[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134775, 15011)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "trn_keep = np.random.rand(len(en_ids))>0.1\n",
    "en_trn,j_trn = en_ids[trn_keep],j_ids[trn_keep]\n",
    "en_val,j_val = en_ids[~trn_keep],j_ids[~trn_keep]\n",
    "len(en_trn),len(en_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd.variable import Variable\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, x, y): self.x,self.y = x,y\n",
    "    def __getitem__(self, idx): return A(self.x[idx]), A(self.y[idx])\n",
    "    def __len__(self): return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = Seq2SeqDataset(en_trn,j_trn)\n",
    "val_ds = Seq2SeqDataset(en_val,j_val)\n",
    "\n",
    "bs = 120\n",
    "\n",
    "trn_dl = DataLoader(trn_ds,batch_size=bs,shuffle=True)\n",
    "val_dl = DataLoader(val_ds,batch_size=int(bs*1.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([120, 25]), torch.Size([120, 25]))"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(trn_dl))\n",
    "x.size(), y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/paperspace/projects/jadlg_rnn/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/home/paperspace/projects/jadlg_rnn/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/home/paperspace/projects/jadlg_rnn/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/paperspace/projects/jadlg_rnn/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/paperspace/projects/jadlg_rnn/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "enlen_90 = int(np.percentile([len(o) for o in en_ids], 99))\n",
    "jlen_90 = int(np.percentile([len(o) for o in j_ids], 97))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 22)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enlen_90,jlen_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_ids = pad_sequences(j_ids, maxlen=25, dtype='int32', padding='post', truncating='post', value=1)\n",
    "en_ids = pad_sequences(en_ids, maxlen=25, dtype='int32', padding='post', truncating='post', value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load fasttext vectors\n",
    "import io\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    header = fin.readline().split()\n",
    "    n, d = int(header[0]), int(header[1])\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.array(tokens[1:], dtype=float)\n",
    "    return data, n, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
    "#!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ja.300.vec.gz\n",
    "#!unzip wiki-news-300d-1M.vec.zip\n",
    "#!gunzip cc.ja.300.vec.gz\n",
    "#mv wiki-news-300d-1M.vec ../data/\n",
    "#mv cc.ja.300.vec ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vecs,_,dim_en_vec = load_vectors('../data/wiki-news-300d-1M.vec')\n",
    "j_vecs,_,dim_j_vec = load_vectors('../data/cc.ja.300.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb(vecs, itos, em_sz):\n",
    "    emb = nn.Embedding(len(itos), em_sz, padding_idx=1)\n",
    "    wgts = emb.weight.data\n",
    "    miss = []\n",
    "    for i,w in enumerate(itos):\n",
    "        try: wgts[i] = torch.from_numpy(vecs[w])\n",
    "        except: miss.append(w)\n",
    "    print('Number of unknowns in data: {}'.format(len(miss)))\n",
    "    return emb\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def V(tensor):\n",
    "    if torch.cuda.is_available():return Variable(tensor.cuda())\n",
    "    else: return Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,en_vecs,int2en,j_vecs,int2j,em_sz,nh=128,out_sl=25,dropf=1,nl=2):\n",
    "        super().__init__()\n",
    "        #encoder\n",
    "        self.nl,self.nh,self.em_sz,self.out_sl = nl,nh,em_sz,out_sl\n",
    "        self.emb_enc = create_emb(en_vecs,int2en,dim_en_vec)\n",
    "        self.emb_drop = nn.Dropout(0.15*dropf)\n",
    "        self.encoder = nn.GRU(dim_en_vec,nh,num_layers=nl,dropout=0.25*dropf, bidirectional=True,batch_first=True)\n",
    "        #decoder\n",
    "        self.emb_dec = create_emb(j_vecs,int2j,dim_j_vec)\n",
    "        self.decoder = nn.GRU(dim_en_vec,nh*2,num_layers=nl,dropout=0.25*dropf,batch_first=True)\n",
    "        self.out_drop = nn.Dropout(0.35*dropf)\n",
    "        self.out = nn.Linear(nh*2,len(int2j))\n",
    "    \n",
    "    def forward(self,inp,y=None):\n",
    "        bs, sl = inp.size()\n",
    "        emb_in = self.emb_drop(self.emb_enc(inp))\n",
    "        h_n = self.initHidden(bs)\n",
    "        enc_out, h_n = self.encoder(emb_in,h_n)\n",
    "        h_n = h_n.view(2,2,bs,-1).permute(0,2,1,3).contiguous().view(self.nl,bs,-1)\n",
    "        \n",
    "        dec_inp = V(torch.zeros(bs).long())\n",
    "        res = []\n",
    "        for i in range(self.out_sl):\n",
    "            dec_emb = self.emb_dec(dec_inp)\n",
    "            _,h_n = self.decoder(dec_emb.unsqueeze(1),h_n)\n",
    "            outp = self.out_drop(self.out(h_n[-1]))\n",
    "            res.append(outp)\n",
    "            dec_inp = outp.data.max(1)[1]\n",
    "            if (dec_inp==1).all(): break\n",
    "        return torch.stack(res)\n",
    "        \n",
    "    def initHidden(self,bs):\n",
    "        return V(torch.zeros([self.nl*2,bs,self.nh]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unknowns in data: 974\n",
      "Number of unknowns in data: 492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (emb_enc): Embedding(21393, 300, padding_idx=1)\n",
       "  (emb_drop): Dropout(p=0.15)\n",
       "  (encoder): GRU(300, 128, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "  (emb_dec): Embedding(31813, 300, padding_idx=1)\n",
       "  (decoder): GRU(300, 256, num_layers=2, batch_first=True, dropout=0.25)\n",
       "  (out_drop): Dropout(p=0.35)\n",
       "  (out): Linear(in_features=256, out_features=31813, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq = Seq2Seq(en_vecs,int2en,j_vecs,int2j,dim_en_vec)\n",
    "seq2seq.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 64, 31813])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = seq2seq(V(x.long()))\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_loss(input, target):\n",
    "    bs,sl = target.size()\n",
    "    sl_in,bs_in,nc = input.size()\n",
    "    if sl>sl_in: input = F.pad(input, (0,0,0,0,0,sl-sl_in))\n",
    "    input = input[:sl]\n",
    "    return F.cross_entropy(input.view(-1,nc), target.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.3802, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq_loss(out,V(y.long()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(x, y, epoch, m, crit, opt, clip=None):\n",
    "    output = m(x, y)\n",
    "    opt.zero_grad()\n",
    "    loss = crit(output, y)\n",
    "    loss.backward()\n",
    "    if clip:\n",
    "        nn.utils.clip_grad_norm_(m.parameters(), clip)\n",
    "    opt.step()\n",
    "    return loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trn_dl,val_dl,model,crit,opt,epochs=10,clip=None):\n",
    "    for epoch in range(epochs):\n",
    "        with tqdm(total=len(trn_dl)) as pbar:\n",
    "            model.train()\n",
    "            loss_trn = loss_val = 0\n",
    "            for i, ds in enumerate(trn_dl):\n",
    "                x, y = ds\n",
    "                loss = step(V(x.long()),V(y.long()),epoch,model,crit,opt)\n",
    "                loss_trn += loss\n",
    "                pbar.update()\n",
    "        model.eval()\n",
    "        for i, ds in enumerate(val_dl):\n",
    "            x, y = ds\n",
    "            out = model(V(x.long()))\n",
    "            loss = crit(output, V(y.long()))\n",
    "            loss_val +=loss\n",
    "        print(f'Epoch: {epoch} trn loss: {loss_trn/trn_dl.batch_size} val loss: {loss_val/val_dl.batch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(seq2seq.parameters(),lr=3e-3,betas=(0.7,0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 545/1124 [01:29<01:35,  6.09it/s]"
     ]
    }
   ],
   "source": [
    "train(trn_dl,val_dl,seq2seq,seq2seq_loss,opt,epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_out(val_dl, model,int2en,int2j,interval=(20,30)):\n",
    "    x,y = next(iter(val_dl))\n",
    "    probs = seq2seq(V(x))\n",
    "    preds = A(probs.max(2)[1])\n",
    "    for i in range(interval[0],interval[1]):\n",
    "        print(' '.join([int2en[o] for o in x[i,:] if o != 1]))\n",
    "        print(' '.join([int2j[o] for o in y[i,:] if o != 1]))\n",
    "        print(' '.join([int2j[o] for o in preds[i,:] if o!=1]))\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
