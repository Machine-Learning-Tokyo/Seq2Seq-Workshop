{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "translate-colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Machine-Learning-Tokyo/Seq2Seq-Workshop/blob/master/notebooks/translate_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sg68qiQrofYP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Machine Translation\n",
        "A sequence-to-sequence model is a model that takes a sequence of items and outputs another sequence of items using two networks that are trained end-to-end. This is perfect for machine translation since input sequences are directly related to output sequences. We will looking at preparing a dataset for Machine Translation task and implementing a seq2seq model. We will be using the parallel corpus available from [here](ftp://ftp.monash.edu/pub/nihongo/examples.utf.gz) \n",
        "\n",
        "### Concepts covered here:\n",
        "- Encoder-Decoder architecture: We will be using a encoder to encode input sequences from one language in a latent space and use the encoding to generate words in the target language one token at a time. The encoder and the decoder are an RNNs. \n",
        "\n",
        "- Attention: We will be looking at the attention mechanism described in [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf). We will explore the theory, the intuition behind it and how to move from theory and intuition to implementation."
      ]
    },
    {
      "metadata": {
        "id": "K1HzG6z-ofYV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from pathlib import Path\n",
        "import re,string\n",
        "import numpy as np\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U3BP_G6DofYi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MButr8N2ofYo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path = Path('data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pwynv43oofYz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will need to download the translation corpus and install a tokenizer for Japanese text."
      ]
    },
    {
      "metadata": {
        "id": "U61Hl7vSofY1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#get corpus\n",
        "#!wget ftp://ftp.monash.edu/pub/nihongo/examples.utf.gz\n",
        "#decompress it and move it to data folder\n",
        "#!gunzip examples.utf.gz\n",
        "#!mv examples.utf data/\n",
        "#install dependencies for mecab tokenizer\n",
        "#!sudo apt install swig\n",
        "#!sudo apt install mecab\n",
        "#!sudo apt install libmecab-dev\n",
        "#!sudo apt install mecab-ipadic-utf8\n",
        "#!sudo pip3 install mecab-python3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Twukc-eCofY7",
        "colab_type": "code",
        "outputId": "5c32d32f-c2f2-4f0c-8cc3-17cac87076f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "corpus = (path/'examples.utf').open().readlines()\n",
        "corpus[:5]"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A: ムーリエルは２０歳になりました。\\tMuiriel is 20 now.#ID=1282_4707\\n',\n",
              " 'B: は 二十歳(はたち){２０歳} になる[01]{になりました}\\n',\n",
              " 'A: すぐに戻ります。\\tI will be back soon.#ID=1284_4709\\n',\n",
              " 'B: 直ぐに{すぐに} 戻る{戻ります}\\n',\n",
              " 'A: すぐに諦めて昼寝をするかも知れない。\\tI may give up soon and just nap instead.#ID=1300_4727\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "metadata": {
        "id": "nTkXpahLofZG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_corpus(corpus_path):\n",
        "    corpus = corpus_path.open().readlines()\n",
        "    en,ja = [],[]\n",
        "    pat = r'#ID.+\\n'\n",
        "    for c in corpus:\n",
        "        if 'A: ' in c:\n",
        "            clean_c = c.replace('A: ','')\n",
        "            res = re.search(pat,clean_c)\n",
        "            clean_c = clean_c.replace(res.group(0),'').split('\\t')\n",
        "            ja.append(clean_c[0])\n",
        "            en.append(clean_c[1])\n",
        "    return en,ja"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "opZ65gotofZR",
        "colab_type": "code",
        "outputId": "14660e5f-844f-49e5-d971-f463e2070e97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "en, ja = make_corpus(path/'examples.utf')\n",
        "en[:2],ja[:2]"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Muiriel is 20 now.', 'I will be back soon.'],\n",
              " ['ムーリエルは２０歳になりました。', 'すぐに戻ります。'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "metadata": {
        "id": "iR594C8GofZZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import MeCab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3HyMAMlhofZn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tagger = MeCab.Tagger('-Owakati')\n",
        "\n",
        "def ja_tokenizer(text):\n",
        "    result = tagger.parse(text)\n",
        "    words = result.split()\n",
        "    if len(words) ==0: return []\n",
        "    if words[-1] == '\\n':return words[:-1]\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yNT8-DYNofZx",
        "colab_type": "code",
        "outputId": "1024a4b2-4774-447e-e0c4-dbf496d14229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "ja_tokenizer(ja[0])"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ムーリエル', 'は', '２', '０', '歳', 'に', 'なり', 'まし', 'た', '。']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "metadata": {
        "id": "YGocE3VGofZ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.symbols import ORTH"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LXDkYBV9ofZ7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "en_tok = spacy.load('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sr3MSI5nofaC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def en_tokenizer(text):\n",
        "    text = text.lower()\n",
        "    return [t.text for t in en_tok.tokenizer(text)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N7peSCuvofaH",
        "colab_type": "code",
        "outputId": "22f04b39-519e-4d20-baab-79c5b8ce86dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "en_tokenizer(en[0])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['muiriel', 'is', '20', 'now', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "metadata": {
        "id": "QF2BEH-2ofaQ",
        "colab_type": "code",
        "outputId": "855ceeb3-3774-444b-8ea8-2b659c45f0e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "cell_type": "code",
      "source": [
        "en_toks = [en_tokenizer(text) for text in en]\n",
        "ja_toks = [ja_tokenizer(text) for text in ja]\n",
        "en_toks[:2], ja_toks[:2]"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([['muiriel', 'is', '20', 'now', '.'],\n",
              "  ['i', 'will', 'be', 'back', 'soon', '.']],\n",
              " [['ムーリエル', 'は', '２', '０', '歳', 'に', 'なり', 'まし', 'た', '。'],\n",
              "  ['すぐ', 'に', '戻り', 'ます', '。']])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "metadata": {
        "id": "xrqazu4HofaY",
        "colab_type": "code",
        "outputId": "7ff8d3f5-43d6-44a8-d0df-11b91dd5868c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "len(en_toks), len(ja_toks)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(149785, 149785)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "metadata": {
        "id": "Wpfm_Ej7ofap",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter,defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1NJyFRxpofax",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def numericalize_tok(tokens, max_vocab=50000, min_freq=0, unk_tok=\"xxunk\", pad_tok=\"xxpad\", bos_tok=\"xxbos\", eos_tok=\"xxeos\"):\n",
        "    if isinstance(tokens, str):\n",
        "        raise ValueError(\"Expected to receive a list of tokens. Received a string instead\")\n",
        "    if isinstance(tokens[0], list):\n",
        "        tokens = [p for o in tokens for p in o]\n",
        "    freq = Counter(tokens)\n",
        "    int2tok = [o for o,c in freq.most_common(max_vocab) if c>min_freq]\n",
        "    unk_id = 3\n",
        "    int2tok.insert(0, bos_tok)\n",
        "    int2tok.insert(1, pad_tok)\n",
        "    int2tok.insert(2, eos_tok)\n",
        "    int2tok.insert(unk_id, unk_tok)\n",
        "    tok2int = defaultdict(lambda:unk_id, {v:k for k,v in enumerate(int2tok)})\n",
        "    return int2tok, tok2int"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CK8GKEHBofa8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "int2j,j2int = numericalize_tok(ja_toks)\n",
        "int2en,en2int = numericalize_tok(en_toks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5_dgvcf8ofbD",
        "colab_type": "code",
        "outputId": "b5a3780b-446b-4724-b290-4f841b9ddbb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "len(int2j), len(int2en)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31813, 21393)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "metadata": {
        "id": "yoxPO1bsofbK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vM1PpGJrofbP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pickle.dump(int2j,(path/'int2j.pkl').open('wb'))\n",
        "pickle.dump(int2en,(path/'int2en.pkl').open('wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XpnnzchKofbU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "int2j = pickle.load((path/'int2j.pkl').open('rb'))\n",
        "int2en = pickle.load((path/'int2en.pkl').open('rb'))\n",
        "j2int = defaultdict(lambda:3, {v:k for k,v in enumerate(int2j)})\n",
        "en2int = defaultdict(lambda:3, {v:k for k,v in enumerate(int2en)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jhw-qW2kofbZ",
        "colab_type": "code",
        "outputId": "41c623a2-1458-4be9-81d9-3a0a6d2e857f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "len(int2j), len(int2en)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12677, 9290)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "PmPrYkw9ofbo",
        "colab_type": "code",
        "outputId": "0f740131-ba96-47b4-c3b8-93a7133535e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        }
      },
      "cell_type": "code",
      "source": [
        "j_ids = np.array([[0]+[j2int[o] for o in sent]+[2] for sent in ja_toks])\n",
        "en_ids = np.array([[0]+[en2int[o] for o in sent]+[2] for sent in en_toks])\n",
        "len(j_ids),len(en_ids), j_ids[10],en_ids[10]"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(149785,\n",
              " 149785,\n",
              " [0,\n",
              "  48,\n",
              "  6,\n",
              "  4891,\n",
              "  5,\n",
              "  109,\n",
              "  11,\n",
              "  143,\n",
              "  10,\n",
              "  83,\n",
              "  8,\n",
              "  57,\n",
              "  86,\n",
              "  1798,\n",
              "  7,\n",
              "  2146,\n",
              "  232,\n",
              "  255,\n",
              "  47,\n",
              "  36,\n",
              "  4,\n",
              "  2],\n",
              " [0, 114, 2251, 107, 38, 97, 85, 2649, 77, 28, 356, 4, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "metadata": {
        "id": "8C3vDdseofbw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KP5VS6Fbofb2",
        "colab_type": "code",
        "outputId": "044da994-3041-4247-c698-d351ae735b28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "trn_keep = np.random.rand(len(en_ids))>0.1\n",
        "en_trn,j_trn = en_ids[trn_keep],j_ids[trn_keep]\n",
        "en_val,j_val = en_ids[~trn_keep],j_ids[~trn_keep]\n",
        "len(en_trn),len(en_val)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(134774, 15011)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "metadata": {
        "id": "E3X7oW0cofb9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.autograd.variable import Variable\n",
        "from torch.utils.data import Dataset,DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F9RhzGOYofcB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from numpy import array as A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mdg0frrJofcF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Seq2SeqDataset(Dataset):\n",
        "    def __init__(self, x, y): self.x,self.y = x,y\n",
        "    def __getitem__(self, idx): return A(self.x[idx]), A(self.y[idx])\n",
        "    def __len__(self): return len(self.x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bW9NpORdofcI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trn_ds = Seq2SeqDataset(en_trn,j_trn)\n",
        "val_ds = Seq2SeqDataset(en_val,j_val)\n",
        "\n",
        "bs = 120\n",
        "\n",
        "trn_dl = DataLoader(trn_ds,batch_size=bs,shuffle=True)\n",
        "val_dl = DataLoader(val_ds,batch_size=bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "al6bVtbYofcL",
        "colab_type": "code",
        "outputId": "261d0a68-7958-4566-e7a1-bc62db74ad24",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x, y = next(iter(val_dl))\n",
        "x.size(), y.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([120, 25]), torch.Size([120, 25]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "mwU9mMMJofcR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lQKNaYmSofcY",
        "colab_type": "code",
        "outputId": "37536b9e-0a98-48d4-ffba-bd763615d5ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "enlen_90 = int(np.percentile([len(o) for o in en_ids], 99))\n",
        "jalen_90 = int(np.percentile([len(o) for o in j_ids], 99))\n",
        "enlen_90,jalen_90"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25, 29)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "metadata": {
        "id": "LEwZeKxIofch",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "j_ids = pad_sequences(j_ids, maxlen=29, dtype='int32', padding='post', truncating='post', value=1)\n",
        "en_ids = pad_sequences(en_ids, maxlen=25, dtype='int32', padding='post', truncating='post', value=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w1_NIPvGrG1W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9061d69c-08f5-43f1-c5a2-126464172e91"
      },
      "cell_type": "code",
      "source": [
        "trn_keep = np.random.rand(len(en_ids))>0.1\n",
        "en_trn,j_trn = en_ids[trn_keep],j_ids[trn_keep]\n",
        "en_val,j_val = en_ids[~trn_keep],j_ids[~trn_keep]\n",
        "len(en_trn),len(en_val)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(134746, 15039)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "metadata": {
        "id": "pwrMeKyErK3S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trn_ds = Seq2SeqDataset(en_trn,j_trn)\n",
        "val_ds = Seq2SeqDataset(en_val,j_val)\n",
        "\n",
        "bs = 120\n",
        "\n",
        "trn_dl = DataLoader(trn_ds,batch_size=bs,shuffle=True)\n",
        "val_dl = DataLoader(val_ds,batch_size=bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gNxWk9MirNDY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "650fb11d-30b0-42bb-e125-f98ccc90b3e7"
      },
      "cell_type": "code",
      "source": [
        "x, y = next(iter(val_dl))\n",
        "x.size(), y.size()"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([120, 25]), torch.Size([120, 29]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "metadata": {
        "id": "1EAQQbnUofck",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## load fasttext vectors\n",
        "#code from here:https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md\n",
        "import io\n",
        "def load_vectors(fname):\n",
        "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    header = fin.readline().split()\n",
        "    n, d = int(header[0]), int(header[1])\n",
        "    data = {}\n",
        "    for line in fin:\n",
        "        tokens = line.rstrip().split(' ')\n",
        "        data[tokens[0]] = np.array(tokens[1:], dtype=float)\n",
        "    return data, int(n), int(d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "s764uzHPofco",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "a514f24d-a0e9-4fe1-c69c-4222f058ba1c"
      },
      "cell_type": "code",
      "source": [
        "# get word vectors\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ja.300.vec.gz\n",
        "!unzip wiki-news-300d-1M.vec.zip\n",
        "!gunzip cc.ja.300.vec.gz\n",
        "!mv wiki-news-300d-1M.vec data/\n",
        "!mv cc.ja.300.vec data/"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-15 09:27:27--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.22.166, 104.20.6.166, 2606:4700:10::6814:6a6, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.22.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 681808098 (650M) [application/zip]\n",
            "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
            "\n",
            "wiki-news-300d-1M.v 100%[===================>] 650.22M  30.1MB/s    in 22s     \n",
            "\n",
            "2019-03-15 09:27:49 (29.4 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
            "\n",
            "--2019-03-15 09:27:51--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ja.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.22.166, 104.20.6.166, 2606:4700:10::6814:6a6, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.22.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1279641604 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.ja.300.vec.gz’\n",
            "\n",
            "cc.ja.300.vec.gz    100%[===================>]   1.19G  30.1MB/s    in 41s     \n",
            "\n",
            "2019-03-15 09:28:32 (29.9 MB/s) - ‘cc.ja.300.vec.gz’ saved [1279641604/1279641604]\n",
            "\n",
            "Archive:  wiki-news-300d-1M.vec.zip\n",
            "  inflating: wiki-news-300d-1M.vec   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j5-nC9FFofct",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "en_vecs,_,dim_en_vec = load_vectors('data/wiki-news-300d-1M.vec')\n",
        "j_vecs,_,dim_j_vec = load_vectors('data/cc.ja.300.vec')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7HT5m4aRofcx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_emb(vecs, itos, em_sz):\n",
        "    emb = nn.Embedding(len(itos), em_sz, padding_idx=1)\n",
        "    if vecs is None: return emb\n",
        "    wgts = emb.weight.data\n",
        "    miss = []\n",
        "    for i,w in enumerate(itos):\n",
        "        try: wgts[i] = torch.from_numpy(vecs[w])\n",
        "        except: miss.append(w)\n",
        "    print('Number of unknowns in data: {}'.format(len(miss)))\n",
        "    return emb\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yoCxoafyofc3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def V(tensor,req_grad=True):\n",
        "    if torch.cuda.is_available():return Variable(tensor.cuda())\n",
        "    else: return Variable(tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ilfz1MLDofc6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### RNN Visualization"
      ]
    },
    {
      "metadata": {
        "id": "cgnGnKnttPA2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[RNN Visualization](http://jalammar.github.io/images/RNN_1.mp4)"
      ]
    },
    {
      "metadata": {
        "id": "o8zV-dwhofc_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq Architecture\n",
        "> ...a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. \n",
        "\n",
        "[Sutskever et al., 2014](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)"
      ]
    },
    {
      "metadata": {
        "id": "XavH76_gofdC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://lilianweng.github.io/lil-log/assets/images/encoder-decoder-example.png)\n",
        "\n",
        "Image:[from here](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)"
      ]
    },
    {
      "metadata": {
        "id": "gzPOwPQbofdD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,int2en,int2j,em_sz,j_vecs=None,en_vecs=None,nh=128,out_sl=25,dropf=1,nl=2):\n",
        "        super().__init__()\n",
        "        #encoder\n",
        "        self.nl,self.nh,self.em_sz,self.out_sl = nl,nh,em_sz,out_sl\n",
        "        self.emb_enc = create_emb(en_vecs,int2en,em_sz)\n",
        "        self.emb_drop = nn.Dropout(0.15*dropf)\n",
        "        self.encoder = nn.GRU(em_sz,nh,num_layers=nl,dropout=0.25*dropf, bidirectional=True)\n",
        "        #decoder\n",
        "        self.emb_dec = create_emb(j_vecs,int2j,em_sz)\n",
        "        self.decoder = nn.GRU(em_sz,nh*2,num_layers=nl,dropout=0.25*dropf)\n",
        "        self.out_drop = nn.Dropout(0.35*dropf)\n",
        "        self.out = nn.Linear(nh*2,len(int2j))\n",
        "    \n",
        "    def forward(self,inp,y=None):\n",
        "        sl, bs = inp.size()\n",
        "        emb_in = self.emb_drop(self.emb_enc(inp))\n",
        "        h_n = self.initHidden(bs)\n",
        "        enc_out, h_n = self.encoder(emb_in,h_n)\n",
        "        h_n = h_n.view(2,2,bs,-1).permute(0,2,1,3).contiguous().view(self.nl,bs,-1)\n",
        "        \n",
        "        dec_inp = V(torch.zeros(bs).long())\n",
        "        res = []\n",
        "        for i in range(self.out_sl):\n",
        "            dec_emb = self.emb_dec(dec_inp)\n",
        "            outp,h_n = self.decoder(dec_emb.unsqueeze(0),h_n)\n",
        "            outp = F.log_softmax(self.out(self.out_drop(outp[0])),dim=-1)\n",
        "            res.append(outp)\n",
        "            dec_inp = outp.data.max(1)[1]\n",
        "            if (dec_inp==1).all(): break\n",
        "        return torch.stack(res)\n",
        "        \n",
        "    def initHidden(self,bs):\n",
        "        return V(torch.zeros([self.nl*2,bs,self.nh]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X-ICi6rIofdH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Why Bidirectional Encoder\n",
        "> Finally, we found that reversing the order of the words in all source sentences (butnot target sentences) improved the LSTM’s performance markedly, because doing so introduced many short term dependencies between the source and the targetsentence which made the optimization problem easier.\n",
        "\n",
        "[Sutskever et al., 2014](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)\n",
        "\n",
        "Input and output sequences may not directly map to each other so preserving information from both passes of the input sequence will help learn how tokens relate to each other. For example in a translation task, subject and object can be in opposite positions depending on the language structure."
      ]
    },
    {
      "metadata": {
        "id": "zMiSmH-1ofdI",
        "colab_type": "code",
        "outputId": "7b4ba92d-49aa-4b1d-c4de-6f16ac2c8183",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "cell_type": "code",
      "source": [
        "seq2seq = Seq2Seq(int2en,int2j,300,en_vecs=None,j_vecs=None)\n",
        "seq2seq.cuda()"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (emb_enc): Embedding(21393, 300, padding_idx=1)\n",
              "  (emb_drop): Dropout(p=0.15)\n",
              "  (encoder): GRU(300, 128, num_layers=2, dropout=0.25, bidirectional=True)\n",
              "  (emb_dec): Embedding(31813, 300, padding_idx=1)\n",
              "  (decoder): GRU(300, 256, num_layers=2, dropout=0.25)\n",
              "  (out_drop): Dropout(p=0.35)\n",
              "  (out): Linear(in_features=256, out_features=31813, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "HM4wZkm4ofdN",
        "colab_type": "code",
        "outputId": "099cc325-b237-43ed-dd38-9b1579ed1eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "out = seq2seq(V(x.transpose(1,0).long()))\n",
        "out.size()"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([25, 120, 31813])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "metadata": {
        "id": "iBFRG6KquIrA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Loss Function: We will use Cross Entropy Loss as we are trying to classify ot the correct words. Cross entropy loss can be simplified to: \n",
        "\n",
        "`cross_entropy = sum(-log(y_pred) for y_pred in y_preds)`\n",
        "\n",
        "where `y_pred` is the likelihood of the target class predicted by the model. This is a good loss function for classification because if the likelihood of the correct class is low, the loss value goes up and if it is high, the loss value goes down."
      ]
    },
    {
      "metadata": {
        "id": "iVZr0DKXofdg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def seq2seq_loss(input, target):\n",
        "    sl,bs = target.size()\n",
        "    sl_in,bs_in,nc = input.size()\n",
        "    if sl>sl_in: input = F.pad(input, (0,0,0,0,0,sl-sl_in))\n",
        "    input = input[:sl]\n",
        "    return F.cross_entropy(input.view(-1,nc), target.view(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ZiCZXSLofdi",
        "colab_type": "code",
        "outputId": "a58d8d8d-4d0b-49ba-8714-e46f644e0c7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "seq2seq_loss(out,V(y.transpose(1,0).long()))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(10.3560, device='cuda:0', grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "metadata": {
        "id": "sgZkBJpmofdu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def step(x, y, epoch, m, crit, opt, clip=None):\n",
        "    output = m(x, y)\n",
        "    if isinstance(output,tuple): output = output[0]\n",
        "    opt.zero_grad()\n",
        "    loss = crit(output, y)\n",
        "    loss.backward()\n",
        "    if clip:\n",
        "        nn.utils.clip_grad_norm_(m.parameters(), clip)\n",
        "    opt.step()\n",
        "    return loss.data.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U9etuKhJofdx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BfqS-K9hofd0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(trn_dl,val_dl,model,crit,opt,epochs=10,clip=None):\n",
        "    for epoch in range(epochs):\n",
        "        loss_val = loss_trn = 0\n",
        "        with tqdm(total=len(trn_dl)) as pbar:\n",
        "            model.train()\n",
        "            for i, ds in enumerate(trn_dl):\n",
        "                x, y = ds\n",
        "                #if isinstance(x,tuple): x = x[0]\n",
        "                x, y = x.transpose(1,0), y.transpose(1,0)\n",
        "                loss = step(V(x.long()),V(y.long()),epoch,model,crit,opt)\n",
        "                loss_trn += loss\n",
        "                pbar.update()\n",
        "        model.eval()\n",
        "        for i, ds in enumerate(val_dl):\n",
        "            with torch.no_grad():\n",
        "                x, y = ds\n",
        "                #if isinstance(x,tuple): x = x[0]\n",
        "                x, y = x.transpose(1,0), y.transpose(1,0)\n",
        "                out = model(V(x.long()))\n",
        "                if isinstance(out,tuple): out = out[0]\n",
        "                loss_val+= crit(out, V(y.long()))\n",
        "                #loss_val +=loss\n",
        "        print(f'Epoch: {epoch} trn loss: {loss_trn/len(trn_dl)} val loss: {loss_val/len(val_dl)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TjyftC-xofd4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch import optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zR4cgUcuofd9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "opt = optim.Adam(seq2seq.parameters(),lr=3e-3,betas=(0.7,0.8))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "jHRr5_-eofeE",
        "colab_type": "code",
        "outputId": "42e4d3bf-ee1b-408c-9601-ea8db2944cbb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train(trn_dl,val_dl,seq2seq,seq2seq_loss,opt,epochs=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1124/1124 [02:40<00:00,  7.02it/s]\n",
            "  0%|          | 1/1124 [00:00<03:05,  6.06it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 trn loss: 4.750732962983359 val loss: 4.082951068878174\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1124/1124 [02:52<00:00,  6.51it/s]\n",
            "  0%|          | 1/1124 [00:00<03:11,  5.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 trn loss: 3.5997307712073003 val loss: 3.3246185779571533\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1124/1124 [02:55<00:00,  6.39it/s]\n",
            "  0%|          | 1/1124 [00:00<03:04,  6.08it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 2 trn loss: 3.1077632744965604 val loss: 3.4751904010772705\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1124/1124 [02:57<00:00,  6.34it/s]\n",
            "  0%|          | 1/1124 [00:00<02:50,  6.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 3 trn loss: 2.8518904808153036 val loss: 3.94274640083313\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1124/1124 [02:58<00:00,  6.30it/s]\n",
            "  0%|          | 1/1124 [00:00<03:06,  6.01it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 4 trn loss: 2.7045018161743135 val loss: 2.766321897506714\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1124/1124 [02:58<00:00,  6.29it/s]\n",
            "  0%|          | 1/1124 [00:00<03:06,  6.01it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5 trn loss: 2.5743112220458713 val loss: 2.752124071121216\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1124/1124 [02:58<00:00,  6.28it/s]\n",
            "  0%|          | 1/1124 [00:00<03:04,  6.08it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 6 trn loss: 2.5332621384769998 val loss: 2.935807228088379\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1124/1124 [02:59<00:00,  6.27it/s]\n",
            "  0%|          | 1/1124 [00:00<03:03,  6.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 7 trn loss: 2.500852593323514 val loss: 3.133931875228882\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1124/1124 [02:59<00:00,  6.99it/s]\n",
            "  0%|          | 1/1124 [00:00<03:04,  6.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 8 trn loss: 2.458315055141245 val loss: 2.8610451221466064\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1124/1124 [02:59<00:00,  6.26it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 9 trn loss: 2.456163497369909 val loss: 3.016350269317627\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EXi-KaFMofeI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def produce_out(val_dl, model,int2en,int2j,interval=(20,30)):\n",
        "    model.eval()\n",
        "    x,y = next(iter(val_dl))\n",
        "    x, y = x.transpose(1,0), y.transpose(1,0)\n",
        "    probs = seq2seq(V(x.long()))\n",
        "    if isinstance(probs,tuple): probs = probs[0] \n",
        "    preds = A(probs.max(2)[1].cpu())\n",
        "    for i in range(interval[0],interval[1]):\n",
        "        print(' '.join([int2en[o] for o in x[:,i] if o not in [0,1,2]]))\n",
        "        print(''.join([int2j[o] for o in y[:,i] if o not in [0,1,2]]))\n",
        "        print(''.join([int2j[o] for o in preds[:,i] if o not in [0,1,2]]))\n",
        "        print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "lQeV6Kr3ofeM",
        "colab_type": "code",
        "outputId": "852c4647-9a50-4ced-8181-cd1463df77c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        }
      },
      "cell_type": "code",
      "source": [
        "produce_out(trn_dl,seq2seq,int2en,int2j)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the flower garden needs watering .\n",
            "その花壇は水をやる必要がある。\n",
            "その花は庭がががが。\n",
            "\n",
            "country life is very peaceful in comparison with city life .\n",
            "田舎での生活は、都会生活と比較してとても穏やかだ。\n",
            "国ののはははのののににに。。\n",
            "\n",
            "you are beautifully dressed .\n",
            "あなたはとても美しいドレスを着ていらっしゃいますね。\n",
            "あなたははなををててている。\n",
            "\n",
            "it 's quite all right .\n",
            "全くかまいません。\n",
            "それはですだ。\n",
            "\n",
            "the president put off visiting japan .\n",
            "大統領は訪日を延期しました。\n",
            "大統領は日本にををしたた\n",
            "\n",
            "he has two brothers , one lives in osaka and the other in kobe .\n",
            "彼には兄弟が二人いて、一人は大阪で、もう一人は神戸で暮らしている。\n",
            "彼はは２人２人ががが、人人でで。。\n",
            "\n",
            "a famous architect built this house .\n",
            "有名な建築家がこの家を建てた。\n",
            "そのな家家家家家家をたた。\n",
            "\n",
            "i could not but think that he had died .\n",
            "彼は死んでしまったと考えざるを得なかった。\n",
            "私は彼たたたはははたた。\n",
            "\n",
            "i enjoyed watching the easter parade .\n",
            "私は復活祭のパレードを見て楽しんだ。\n",
            "私はそのをを見を見を見。。\n",
            "\n",
            "i will not allow you to be ill - treated .\n",
            "君が虐待されているのを放ってはいられない。\n",
            "私は病気にににににない。。。\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UzHLtvebofeQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.save(seq2seq.state_dict(),open(path/'translate_seq2seq.pth','wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2hUfDEl5ofeT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seq2seq.load_state_dict(torch.load('translate_seq2seq.pth', map_location=lambda storage, loc: storage))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qgdyPPy0ofeU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq w/ Attention\n",
        "A critical and apparent disadvantage of this fixed-length context vector design is incapability of remembering long sentences. Often it has forgotten the first part once it completes processing the whole input. The attention mechanism was born [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) to resolve this problem."
      ]
    },
    {
      "metadata": {
        "id": "98fQ-RqEofeV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Given the following vectors:\n",
        "\\begin{align}\n",
        "\\boldsymbol{x} = \\{x_1,x_2,x_3,\\ldots,x_n\\} \\\\\n",
        "\\boldsymbol{y} = \\{y_1,y_2,y_3,\\ldots,y_m\\} \\\\\n",
        "\\end{align}\n",
        "The hidden state from the Bidir encoder is given by:\n",
        "\\begin{align}\n",
        "\\boldsymbol{h}_i = [\\overrightarrow{\\boldsymbol{h}}_i^\\top; \\overleftarrow{\\boldsymbol{h}}_i^\\top]^\\top, i=1,\\dots,n \\\\\n",
        "\\end{align}\n",
        "\n",
        "The hidden state from the decoder at time $t$ is given by:  $s_t$\n",
        "\n",
        "The score for at time $t$:\n",
        "\n",
        "\\begin{aligned}\n",
        "\\text{score}(\\boldsymbol{s}_t, \\boldsymbol{h}_i) = \\mathbf{v}_a^\\top \\tanh(\\mathbf{W}_a[\\boldsymbol{s}_t; \\boldsymbol{h}_i])\n",
        "\\end{aligned}\n",
        "\n",
        "where both $v_a$ and $W_a$ are weight matrices to be learned in the alignment model.\n",
        "\n",
        "\\begin{aligned}\n",
        "\\mathbf{c}_t &= \\sum_{i=1}^n \\alpha_{t,i} \\boldsymbol{h}_i & \\small{\\text{; Context vector for output }y_t}\\\\\n",
        "\\alpha_{t,i} &= \\text{align}(y_t, x_i) & \\small{\\text{; How well two words }y_t\\text{ and }x_i\\text{ are aligned.}}\\\\\n",
        "&= \\frac{\\exp(\\text{score}(\\boldsymbol{s}_{t-1}, \\boldsymbol{h}_i))}{\\sum_{i'=1}^n \\exp(\\text{score}(\\boldsymbol{s}_{t-1}, \\boldsymbol{h}_{i'}))} & \\small{\\text{; Softmax of some predefined alignment score.}}.\n",
        "\\end{aligned}\n",
        "\n",
        "Equations borrowed from [here](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "m214-pzUvqKk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Attention Visualization](http://jalammar.github.io/images/attention_process.mp4)"
      ]
    },
    {
      "metadata": {
        "id": "0unSAxc9ofeh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math,random\n",
        "\n",
        "def rand_t(*sz): return torch.randn(sz)/math.sqrt(sz[0])\n",
        "def rand_p(*sz): return nn.Parameter(rand_t(*sz))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0mswRcrEofej",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Seq2SeqAttention(nn.Module):\n",
        "    def __init__(self,int2en,int2j,em_sz,j_vecs=None,en_vecs=None,nh=128,out_sl=25,dropf=1,nl=2):\n",
        "        super().__init__()\n",
        "        #encoder\n",
        "        self.nl,self.nh,self.em_sz,self.out_sl = nl,nh,em_sz,out_sl\n",
        "        self.emb_enc = create_emb(en_vecs,int2en,em_sz)\n",
        "        self.emb_drop = nn.Dropout(0.15*dropf)\n",
        "        self.encoder = nn.GRU(em_sz,nh,num_layers=nl,dropout=0.25*dropf, bidirectional=True)\n",
        "        #decoder\n",
        "        self.emb_dec = create_emb(j_vecs,int2j,em_sz)\n",
        "        self.decoder = nn.GRU(em_sz,nh*2,num_layers=nl,dropout=0.25*dropf)\n",
        "        self.out_drop = nn.Dropout(0.35*dropf)\n",
        "        self.out = nn.Linear(nh*2,len(int2j))\n",
        "        #attention layer\n",
        "        self.W1 = rand_p(nh*2, nh*2) #parameter\n",
        "        self.l2 = nn.Linear(nh*2, nh*2)\n",
        "        self.l3 = nn.Linear(em_sz+nh*2, em_sz)\n",
        "        self.V = rand_p(nh*2) #parameter\n",
        "    \n",
        "    def forward(self,inp,y=None):\n",
        "        sl, bs = inp.size()\n",
        "        emb_in = self.emb_drop(self.emb_enc(inp))\n",
        "        h_n = self.initHidden(bs)\n",
        "        enc_out, h_n = self.encoder(emb_in,h_n)\n",
        "        h_n = h_n.view(2,2,bs,-1).permute(0,2,1,3).contiguous().view(self.nl,bs,-1)\n",
        "        \n",
        "        dec_inp = V(torch.zeros(bs).long())\n",
        "        res,attns = [], []\n",
        "        #multiply by parameter\n",
        "        w1e = enc_out @ self.W1\n",
        "        for i in range(self.out_sl):\n",
        "            #linear layer \n",
        "            w2h = self.l2(h_n[-1])\n",
        "            #non-linear activation to calculate score\n",
        "            u = torch.tanh(w1e + w2h)\n",
        "            #softmax to make them into probs\n",
        "            a = F.softmax(u @ self.V, 0)\n",
        "            attns.append(a)\n",
        "            #multiply each vector by scores and then add them up\n",
        "            Xa = (a.unsqueeze(2) * enc_out).sum(0)\n",
        "            dec_emb = self.emb_dec(dec_inp)\n",
        "            #linear layer to reduce dimensions\n",
        "            wgt_enc = self.l3(torch.cat([dec_emb, Xa], 1))\n",
        "            outp,h_n = self.decoder(wgt_enc.unsqueeze(0),h_n)\n",
        "            outp = F.log_softmax(self.out(self.out_drop(outp[0])),dim=-1)\n",
        "            res.append(outp)\n",
        "            dec_inp = outp.data.max(1)[1]\n",
        "            if (random.random() > 0.5) and y is not None: dec_inp=y[i] \n",
        "            if (dec_inp==1).all(): break\n",
        "        return torch.stack(res),attns\n",
        "        \n",
        "    def initHidden(self,bs):\n",
        "        return V(torch.zeros([self.nl*2,bs,self.nh]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HmHhbq8rofek",
        "colab_type": "code",
        "outputId": "f877476c-9fc2-42cf-9ad8-1059f8e8e9b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "cell_type": "code",
      "source": [
        "seq2seq = Seq2SeqAttention(int2en,int2j,300,en_vecs=None,j_vecs=None)\n",
        "seq2seq.cuda()"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2SeqAttention(\n",
              "  (emb_enc): Embedding(21393, 300, padding_idx=1)\n",
              "  (emb_drop): Dropout(p=0.15)\n",
              "  (encoder): GRU(300, 128, num_layers=2, dropout=0.25, bidirectional=True)\n",
              "  (emb_dec): Embedding(31813, 300, padding_idx=1)\n",
              "  (decoder): GRU(300, 256, num_layers=2, dropout=0.25)\n",
              "  (out_drop): Dropout(p=0.35)\n",
              "  (out): Linear(in_features=256, out_features=31813, bias=True)\n",
              "  (l2): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (l3): Linear(in_features=556, out_features=300, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "metadata": {
        "id": "l7puJEQ7ofer",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "opt = optim.Adam(seq2seq.parameters(),lr=3e-3,betas=(0.7,0.8))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "ynhTylNpofe7",
        "colab_type": "code",
        "outputId": "30d427fc-9b26-4366-fc10-051eb0b85684",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train(trn_dl,val_dl,seq2seq,seq2seq_loss,opt,epochs=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1124/1124 [03:21<00:00,  6.28it/s]\n",
            "  0%|          | 1/1124 [00:00<03:27,  5.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 trn loss: 2.797933133683595 val loss: 4.138897895812988\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1124/1124 [03:26<00:00,  6.24it/s]\n",
            "  0%|          | 1/1124 [00:00<03:32,  5.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 trn loss: 2.16145394790215 val loss: 2.8873095512390137\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1124/1124 [03:26<00:00,  6.17it/s]\n",
            "  0%|          | 1/1124 [00:00<03:29,  5.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 2 trn loss: 2.0703822573733075 val loss: 3.0309219360351562\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1124/1124 [03:26<00:00,  6.07it/s]\n",
            "  0%|          | 1/1124 [00:00<03:28,  5.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 3 trn loss: 2.07098806148322 val loss: 2.844566822052002\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1124/1124 [03:26<00:00,  6.07it/s]\n",
            "  0%|          | 1/1124 [00:00<03:28,  5.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 4 trn loss: 2.088127380906475 val loss: 2.8696582317352295\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1124/1124 [03:26<00:00,  6.26it/s]\n",
            "  0%|          | 1/1124 [00:00<03:29,  5.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5 trn loss: 2.093448414603162 val loss: 2.7895729541778564\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1124/1124 [03:26<00:00,  6.12it/s]\n",
            "  0%|          | 1/1124 [00:00<03:29,  5.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 6 trn loss: 2.1055155513125383 val loss: 2.9072670936584473\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1124/1124 [03:26<00:00,  5.45it/s]\n",
            "  0%|          | 1/1124 [00:00<03:30,  5.34it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 7 trn loss: 2.11192792569191 val loss: 2.7808303833007812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1124/1124 [03:26<00:00,  6.23it/s]\n",
            "  0%|          | 1/1124 [00:00<03:29,  5.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 8 trn loss: 2.116793169139543 val loss: 2.8389084339141846\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1124/1124 [03:26<00:00,  6.13it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 9 trn loss: 2.1336968614325404 val loss: 2.8806753158569336\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "pTCVj6mZoffA",
        "colab_type": "code",
        "outputId": "addf4186-33a1-41ff-a092-d181625f34f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        }
      },
      "cell_type": "code",
      "source": [
        "produce_out(trn_dl,seq2seq,int2en,int2j)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mingle your joys sometimes with your earnest occupation .\n",
            "ときに君の喜びと君の真剣な職業とを交流せしめよ。\n",
            "あなたの仕事をはにはををする。\n",
            "\n",
            "a tunnel has been bored through the mountain .\n",
            "山を掘り抜いてトンネルが造られた。\n",
            "そのはは山ののをた。\n",
            "\n",
            "please refrain from smoking .\n",
            "どうぞタバコを控えてください。\n",
            "タバコをタバコをください。\n",
            "\n",
            "i am poor at tennis .\n",
            "私はテニスが下手だ。\n",
            "私はテニスがテニスが。\n",
            "\n",
            "i still owe my brother the ten dollars that he lent me last week .\n",
            "先週弟が貸してくれた１０ドル、借りたままだ。\n",
            "私は私ののののののののを１週間、ていた。\n",
            "\n",
            "there is a rumor that she got married .\n",
            "彼女が結婚したといううわさがある。\n",
            "彼女は結婚結婚した。\n",
            "\n",
            "he tried to appeal .\n",
            "彼は訴えようとした。\n",
            "彼はうとした。\n",
            "\n",
            "day is breaking .\n",
            "夜が明けかけてきた。\n",
            "日は日日。\n",
            "\n",
            "though she looks like his older sister , the fact is that she is his mother .\n",
            "彼女は彼の姉のように見えるが、実は母親なのだ。\n",
            "彼女は彼の妹に、て、、は、彼の。\n",
            "\n",
            "his failure in business left him penniless .\n",
            "彼は事業に失敗して一文なしになった。\n",
            "彼の失敗は失敗したのは失敗した。\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Vxl-CyUYoffD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.save(seq2seq.state_dict(),open('translate_seq2seq_attention.pth','wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MbRIfxtJoffF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seq2seq.load_state_dict(torch.load('translate_seq2seq_attention.pth', map_location=lambda storage, loc: storage))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uOrfhaCGoffG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "out, atts = seq2seq(x.long().cuda())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tlqy3VhVoffJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GgbCdh-DoffN",
        "colab_type": "code",
        "outputId": "d7efc12c-26fc-432f-91a3-d791f76d0f2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "torch.stack(atts).size()"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([25, 120, 25])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "metadata": {
        "id": "utuQs5KHoffV",
        "colab_type": "code",
        "outputId": "e0d1c1bc-21a2-4f4f-da9b-c318496aff38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "plt.matshow(torch.stack(atts)[:,5,:].detach().cpu().numpy())"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f8c7b969eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD+CAYAAAD1VNNvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlOGXkjL8UlKGX0rq/wDDgRRvzXGMKAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "PH94uE0L4MGr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}